# Docker を用いたコンテナ時代のアプリケーションデプロイ (前半)

## 仮想化技術とは

前半の講義で紹介したとおり、昔のインフラ構築では物理サーバが基本になっており、
かつ、当時は Infrastructure as Code がそこまで普及していなかったため、サーバの構築は一つ一つを手作業で行っていました。

例えば、開発環境・ステージング環境・本番環境のような似たような環境が必要な場合でも、
一つ一つサーバを調達して、サーバを一つ一つセットアップしていたわけです。

また、サーバをスケールアウト・スケールインで増やしたり減らしたりするのも非常に困難で、
購入にも数週間程度、かつ購入すると殆どの場合で使い続ける必要があり減らすのも困難、というような状況でした。

そういった流れの中で誕生したのが、サーバ仮想化技術です。
（実は仮想化技術自体はメインフレーム等の時代からあるが、通常の PC サーバ上でも実行できるようになってきたのが 2000 年代頃）
サーバ仮想化技術とは、1 台の物理サーバ上で、複数の仮想のマシンを起動するような技術を指します。

本来 OS は CPU・ディスク・メモリ等に依存して動作するようなものですが、
仮想化技術では、OS に対して仮想的に CPU やディスクやメモリなどのハードウェアの挙動をソフトウェア的に実装して模倣し、
仮想マシンからはあたかも CPU などのデバイスが存在しているように見せかけることで、1 台のサーバ上で複数の OS を動かす事を可能にしています。
（このように、ハードウェアをソフトウェアで実装して仮想的に用意する手法を、ハードウェア仮想化、と呼んだりする）

このとき、見せかけ上で用意されたハードウェアのことを、仮想マシンや仮想ハードウェアと呼んだりします。
また、仮想マシン上で仮想する OS のことを、ゲストOS と呼んだりします。

このような仮想化技術の発展により、物理サーバのリソースをより有効に活用できたり、
仮想ディスクのイメージをコピーすることで複数の環境を一気に用意出来たり、耐障害性を向上させたりと、様々な恩恵を享受出来るようになりました。

例えば、これまで使ってきた AWS もこの仮想化技術によって成り立っているもので、
莫大な物理サーバ群の上で、ゲスト OS をユーザに提供するという事を行っています。

ちなみに、ハードウェア仮想化には複数の手法があり、物理サーバに直接ハードウェアを仮想化する機構を導入するようなもの（ハイパーバイザ型: Xen, VMware ESXi, vSpere, Hyper-V）や、
物理サーバの上で通常の OS が動き、その OS の上でハードウェアを仮想化する機構を導入するようなもの（ホスト型: VirtualBox, Parallels, KVM/QEMU, VMWare Fusion）などがあります。
（AWS では、最初は Xen がメインで使われており、今では KVM をベースとして独自進化させた Nitro と呼ばれているものも使われているそうです）

## コンテナ技術 (OS 仮想化)

また、別の仮想化技術として、コンテナ型仮想化（OS 仮想化とも呼ばれることがある）も存在します。
コンテナ型仮想化では、ハードウェア仮想化とは違い、サーバ上で稼働している OS は 1 つとなります。

一般的な OS の内部は、ハードウェアとのやりとりを行ったり他のプロセスの管理をしたりする **カーネル空間** と、
実際のアプリケーションが動作する **ユーザ空間** というものに分かれて設計されています。

通常、OS を起動した段階ではユーザ空間は 1 つしか無いが、コンテナ型仮想化では、このユーザ空間を複数に分離し、
それぞれのユーザ空間から見えるリソースを制限するといった方式で仮想化を行っています。

このように、ユーザプロセスを詰め込んで分割されたユーザ空間のことを **コンテナ** と呼びます。
コンテナはそれ自体 Linux の機能ではなく、`cgroups` や `pivot_root` のようなプリミティブな Linux の機能の組み合わせによって実現されます。

コンテナごとに見えるリソースの制限には、例えば次のようなものがあります。

- CPU/メモリ/デバイス
    - コンテナから利用できる量や範囲が制限されています
        - Linux では `cgroups` という機能を使って実装されています
- ファイルシステム
    - コンテナごとに特定のディレクトリをルートディレクトリとして見せるようにします
        - `pivot_root` や `chroot` のような機能を使って実装されています
- ネットワーク
    - コンテナごとに独立したネットワーク設定が行われます
        - `veth` という仮想 NIC の機能や `netns` という機能を使って実装されています
- プロセステーブル
    - コンテナごとに独立したプロセステーブルが用意されてそれが参照される
    - つまりコンテナ内では他のコンテナやホスト OS のプロセスは見えません

実際に、Docker 上で Ubuntu 16.04 のコンテナを動かしてみて、`ps aufxww` や `df -h`, `ip a` などの、
プロセスやディスクやネットワークの状態を見るコマンドを実行してみると、ホスト OS と幾つかの違いがあることに気づけます。

このように、コンテナ型仮想化のコンテナの内部で起動しているプロセスは、OS 上で普通に動いているプロセスと同じものであり、
ハードウェア仮想化のように、コンテナの内部で OS が起動しているわけではありません（頑張れば OS を起動することも出来ますがあまりメリットはないと思う）。

コンテナ型仮想化では、ハードウェア仮想化とくらべて、次のようなメリットがあると言えます。

- OS ごと起動しているわけではないので起動が高速（通常の `fork()` ぐらいの速度で起動する）
- OS 自体が起動しているわけではないのでメモリ効率がよい
- ハードウェアがエミュレーションされているわけではないので互換性や性能と言った点で有利

一方で、コンテナ型仮想化では、ハードウェア仮想化と比べて次のような特徴も持っています。

- ホスト OS と違う OS を動かすことは非常に困難
    - コンテナ型仮想化では OS は一つなので Linux の上で Windows を動かすようなことは難しい
- システムが完全に抽象化されていない
    - コンテナ型仮想化では制限されているだけであくまで同じ OS 上での動作なため、完全に抽象化されていない箇所があります
    - 例えば通常のアプリケーションではなく低レイヤを触るようなコードでは期待しない動作をする可能性もあります

## Docker

コンテナ型仮想化を一気に世界に普及したと言っても過言ではないツールとして、**Docker** があります。

Docker が行っていたことは、特段技術的革新があったというわけでは無いと思いますが、
コンテナ型仮想化を使う上で不便だった様々な事象を解決し、様々なエコシステムを用意する事で、加速度的に普及が進みました。

Docker が登場した当時特徴的だったものとして、Docker イメージの作成・配布というものがありました。

Docker では、前述のコンテナ型仮想化におけるルートディレクトリの分離を行う際に、
Docker イメージと呼ばれる tar ボールを展開した位置に `pivot_root` してルートディレクトリを切り替えるという挙動をします。
(Docker イメージが実際に tar である様子は、`docker save ubuntu:16.04 > ubuntu-16.04.tar` と `tar tvf ubuntu-16.04.tar` などで見れます)

```
➜  ~ ls -asl ubuntu-16.04.tar
228520 -rw-r--r--  1 ryota-yoshikawa  staff  116999680 Apr 18 04:18 ubuntu-16.04.tar
➜  ~ less ubuntu-16.04.tar
"ubuntu-16.04.tar" may be a binary file.  See it anyway?
➜  ~ tar tvf ubuntu-16.04.tar
drwxr-xr-x  0 0      0           0 Apr 13 03:58 a63112fa621f9ec0cd0e4ae0e6dd9b759e6a59a7202688205bb7d7511d5b18ee/
-rw-r--r--  0 0      0           3 Apr 13 03:58 a63112fa621f9ec0cd0e4ae0e6dd9b759e6a59a7202688205bb7d7511d5b18ee/VERSION
-rw-r--r--  0 0      0         464 Apr 13 03:58 a63112fa621f9ec0cd0e4ae0e6dd9b759e6a59a7202688205bb7d7511d5b18ee/json
-rw-r--r--  0 0      0       15872 Apr 13 03:58 a63112fa621f9ec0cd0e4ae0e6dd9b759e6a59a7202688205bb7d7511d5b18ee/layer.tar
drwxr-xr-x  0 0      0           0 Apr 13 03:58 bae2d426b94cc051c20abd6e91662371c75ce9e7d8d77c1f0a62f124e1a332a3/
-rw-r--r--  0 0      0           3 Apr 13 03:58 bae2d426b94cc051c20abd6e91662371c75ce9e7d8d77c1f0a62f124e1a332a3/VERSION
-rw-r--r--  0 0      0        1264 Apr 13 03:58 bae2d426b94cc051c20abd6e91662371c75ce9e7d8d77c1f0a62f124e1a332a3/json
-rw-r--r--  0 0      0        3072 Apr 13 03:58 bae2d426b94cc051c20abd6e91662371c75ce9e7d8d77c1f0a62f124e1a332a3/layer.tar
drwxr-xr-x  0 0      0           0 Apr 13 03:58 c8ddd24d756b50c7af6a7705c5a345c1463ce66b6ea57d587d944d31930d8a04/
-rw-r--r--  0 0      0           3 Apr 13 03:58 c8ddd24d756b50c7af6a7705c5a345c1463ce66b6ea57d587d944d31930d8a04/VERSION
-rw-r--r--  0 0      0         388 Apr 13 03:58 c8ddd24d756b50c7af6a7705c5a345c1463ce66b6ea57d587d944d31930d8a04/json
-rw-r--r--  0 0      0   116936192 Apr 13 03:58 c8ddd24d756b50c7af6a7705c5a345c1463ce66b6ea57d587d944d31930d8a04/layer.tar
-rw-r--r--  0 0      0        3615 Apr 13 03:58 c9d990395902a9e219684af847a63d793ccf72cf2aadeece1b576566c5662400.json
drwxr-xr-x  0 0      0           0 Apr 13 03:58 eb4b49ff4948b3d4e782fb938f7fdb26c3c46021291d1b9b638960e1152cf395/
-rw-r--r--  0 0      0           3 Apr 13 03:58 eb4b49ff4948b3d4e782fb938f7fdb26c3c46021291d1b9b638960e1152cf395/VERSION
-rw-r--r--  0 0      0         464 Apr 13 03:58 eb4b49ff4948b3d4e782fb938f7fdb26c3c46021291d1b9b638960e1152cf395/json
-rw-r--r--  0 0      0       14848 Apr 13 03:58 eb4b49ff4948b3d4e782fb938f7fdb26c3c46021291d1b9b638960e1152cf395/layer.tar
drwxr-xr-x  0 0      0           0 Apr 13 03:58 fc0787c89d909a591cf39696eb88049f9e2fafc5d9ee61e47a94121389adc231/
-rw-r--r--  0 0      0           3 Apr 13 03:58 fc0787c89d909a591cf39696eb88049f9e2fafc5d9ee61e47a94121389adc231/VERSION
-rw-r--r--  0 0      0         464 Apr 13 03:58 fc0787c89d909a591cf39696eb88049f9e2fafc5d9ee61e47a94121389adc231/json
-rw-r--r--  0 0      0        5632 Apr 13 03:58 fc0787c89d909a591cf39696eb88049f9e2fafc5d9ee61e47a94121389adc231/layer.tar
-rw-r--r--  0 0      0         509 Jan  1  1970 manifest.json
-rw-r--r--  0 0      0          88 Jan  1  1970 repositories
```

Docker では、この Docker イメージと呼ばれる tar に対して、アプリケーションのコードだけではなく、
アプリケーションで利用する全てのライブラリなどを同梱する戦略を取ることで、作成したコンテナを別の環境で動かすことを容易にしたり、
アプリケーションの環境に非常に強い再現性を持たせることなどを可能にしました。
（例えば EC2 インスタンスで都度環境構築をするような場合、古いライブラリが apt リポジトリから消えてしまい再現性が失われるなどがあります）

Docker ではこのようなアプリケーションの環境丸ごとのイメージを作るための設定ファイルとして、Dockerfile というものを採用しました。
Dockerfile といっても複雑な書式があるわけではなく、OS 上で実行するコマンドを羅列する程度で済むことが多いです。

例えば、一般的な Rails アプリケーションを動かすような Docker イメージを作るための Docker ファイルは次のようなものになります。

```
FROM ruby:2.5

RUN env DEBIAN_FRONTEND=noninteractive apt-get update && \
    env DEBIAN_FRONTEND=noninteractive apt-get install -y build-essential default-libmysqlclient-dev libxml2-dev zlib1g-dev nodejs

RUN mkdir /app
COPY Gemfile Gemfile.lock /tmp/
RUN cd /tmp && bundle install -j4 --deployment --path /gems

WORKDIR /app
COPY . /app

CMD ["bundle", "exec", "unicorn", "-c", "config/unicorn.conf.rb", "config.ru"]
```

実際に debian 上でサーバを構築するようなコマンドを羅列しただけですが、
このファイルを使ってイメージを作って配布するだけで、 Linux 上の様々な環境でDocker さえ入っていれば Rails を動作させる事が可能になります。

このようにライブラリを含めたアプリケーションコード一式を配布するとなると、配布するファイルサイズが大きくなってしまい、
Docker イメージの配布などに非常に時間が掛かってしまいそうですが、Docker では、イメージをレイヤーという単位に分け、
レイヤー毎に差分管理を行い、差分がある部分だけを変更・配布するようにすることで、同様の Docker イメージの 2 度目の取得や、
共通するレイヤーをもつ Docker イメージの配布・取得がある程度高速に動作するようになっています。

例えば、上記の例で `Gemfile` のみを変更した場合は、その直前の `apt-get update ...` にあたる部分のレイヤーは変更されず、
`COPY Gemfile` 以降のレイヤーが変更されるようになっています。
（`apt-get ...` などの部分は様々なライブラリのインストールにあたりサイズが非常に大きいため、ここをキャッシュできるだけでも大分早くなる）

## オーケストレーション

Docker が加速度的に普及することで、アプリケーションをデプロイするという世界観は大きく変わりました。

これまでは、今回みなさんが研修でやったように、デプロイするアプリケーションの特性に応じてサーバを構築し、
そのサーバに対してアプリケーションコードを配布する、といった手法が一般的でした。

しかし、Docker の登場以降では、アプリケーションのデプロイは、Docker イメージをコピーすることと同義になりました。
（繰り返すようですが Docker イメージはライブラリ含め全部入りなので Docker が動いている Linux にドーンと乗せれば動きます）

これにより、アプリケーションの特性に応じてサーバを構築する必要がなくなり、
サーバを管理する側は、Docker を動かせる環境の Linux を事前に確保しておき、
あとはアプリケーション開発者のみなさんが、アプリケーションを Docker で動作するようにさえしておけば、
事前に何らかの調整（例: xx というアプリを作りたいので yy というライブラリをサーバにインストールしてください！）を行う必要がなく、
アプリケーションを即デプロイできるようになりました。

そのため、インフラを管理する側としては「とにかく大量に Docker が動く Linux  ホストを用意しよう」となるわけです。
しかし、ただ大量に用意しただけでは、どのホストのリソースが空いていて、どのホストではコンテナがたくさん動いていてリソースが足りないのか、
今からデプロイするコンテナは、どこのホストに入れると一番バランスが良いのか、
などのホストごとのリソースを管理しなければならなくなってしまいます
（このように空いてるところにいい感じに入れる技術をスケジューリングと言います）。

また、例えば Docker ホストが障害で落ちてしまった場合に、そのホストで動いていた Docker コンテナを、
他のホストで動かし直してあげる必要などがあります。

このように、Docker ホストを大量に動かしてコンテナを "いい感じ" にすることを、
**コンテナオーケストレーション**、と呼んだりします。

コンテナオーケストレーションツールとして恐らく現在最も有名なものとして、**Kubernetes** などがあります。
Kubernetes では、複数の Docker ホストでクラスタを組み、YAML ファイルでコンテナが何台欲しいかなどを記述することで、
空いているホストに対していい感じにデプロイしてくれたり、ホストが落ちた時はいい感じに別のホストでコンテナを上げなおすなどを行ってくれます。

また、AWS には **Amazon ECS** という、Docker のマネージドオーケストレーションサービスがあります。
ECS には旧来の ecs-agent とよばれるデーモンが動作する EC2 インスタンスを自分で立ち上げてクラスタリングする方式と、**Fargate** とよばれる自分で EC2 インスタンスを用意しなくてもよい 2 つの方式があります。
Fargate のようなクラウドサービスはコンテナ活用の最大の利点でしょう。アプリケーションより下の層はすべてクラウドサービス側で面倒を見てくれるため、アプリ開発者はアプリだけに集中することができます。
いずれの方式も、たとえば「CPU やメモリをこれこれこれだけ確保したコンテナを N 個立ち上げてインターネットから接続できるようにしておく」というような設定をしておくと、
コンテナが不意に落ちたときに自動的に立ち上げ直してくれたり、ロードバランサの下にコンテナをぶら下げて負荷分散してくれたりしてくれます。
今日のハンズオンではコンテナ化の醍醐味である Fargate を利用します。

クックパッドでは、日本では基本的には AWS ECS を利用しており、国外のレシピサービスや関連サービスも徐々に ECS を利用しています。また、実験的に Kubernetes の導入も考えています。
ただし、ECS だけでは、Docker をデプロイする際のスケジューリング以外の様々な作業を行ってくれないため、
より ECS を使いやすくするために、[Hako](https://github.com/eagletmt/hako) という OSS を技術部開発基盤グループの [eagletmt](https://twitter.com/eagletmt) さんが主要メンテナとして開発して利用しています。

Hako では、ECS の設定や、コンテナを何台・どのように設置するかという設定を、Jsonnet で記述することにより、
ECS の設定やデプロイメントを自動で行ってくれます。

また、Hako ではデプロイの前に `script` という設定で何らかの処理を挟むことができ、
例えば ALB の作成・操作や、アプリケーションの前段となる Front コンテナの設定の生成や、
DNS の自動的な設定など、様々な処理を定義して行うことが可能となっています。

Hako の Jsonnet の設定例は次のとおりです。

```
local fileProvider = std.native('provide.file');
local provide(name) = fileProvider(std.toString({ path: 'hello.env' }), name);

{
    scheduler: {
        type: 'ecs',
        region: 'ap-northeast-1',
        cluster: 'eagletmt',
        desired_count: 2,
        task_role_arn: 'arn:aws:iam::012345678901:role/HelloRole',
        deployment_configuration: {
            maximum_percent: 200,
            minimum_healthy_percent: 50,
        },
    },
    app: {
        image: 'ryotarai/hello-sinatra',
        memory: 128,
        cpu: 256,
        links: [
            'redis:redis',
        ],
        env: {
            PORT: '3000',
            MESSAGE: std.format('%s-san', provide('username')),
        },
    },
    additional_containers: {
        front: {
            image_tag: 'hako-nginx',
            memory: 32,
            cpu: 32,
        },
        redis: {
            image_tag: 'redis:3.0',
            cpu: 64,
            memory: 512,
        },
    },
    scripts: [
        (import 'front.libsonnet') + {
            backend_port: 3000,
        },
    ],
}
```

細かい設定値などはこの後行うハンズオンで随時説明していくので、早速ハンズオンを進めていきましょう。

## 参考文献

- http://www.brendangregg.com/blog/2017-11-29/aws-ec2-virtualization-2017.html
- https://www.school.ctc-g.co.jp/columns/nakai/nakai41.html
- https://www.docker.com
- https://github.com/eagletmt/hako
- https://kubernetes.io
- https://aws.amazon.com/ecs/
- [Dockerfile reference](https://docs.docker.com/engine/reference/builder/#usage)
- [Compose file version 3 reference](https://docs.docker.com/compose/compose-file/)
